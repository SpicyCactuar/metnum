El problema de reconocer e identificar im\'agenes es uno que, a simple vista, uno creer\'ia que depende necesariamente del ojo humano. El simple hecho de plantearse c\'omo una computadora pudiese decidir algo del estilo ``esta im\'agen es de un perro'' parece de por s\'i incre\'ible e, inclusive, extremadamente \textit{dif\'icil}.

Lo interesante de las problem\'aticas es que uno puede \textit{encararlas} de una u otra manera que puede abrir el camino para resolverlo. En el caso de asociaci\'on de im\'agenes, uno puede pensarlo como \textit{reconocer} o, en contraposici\'on, \textit{diferenciar} una ilustraci\'on. En este trabajo vamos a atacar el problema (no este sino uno m\'as \textit{acotado}), por ambos caminos para poder arribar a una conclusi\'on.

Dicho esto, poder decidir para \textit{cualquier} tipo de im\'agen, sin ning\'un contexto, a qu\'e objeto o concepto est\'a asociado sigue siendo de dificultad elevada. El an\'alisis del contexto es esencial para cualquier tipo de problema y este no es la excepci\'on.

La situaci\'on concreta a ser abarcada en este trabajo es la de reconocer d\'igitos en im\'agenes con n\'umeros manuscritos con un formato particular:

\begin{itemize}
\item El n\'umero escrito debe ser del 0 al 9
\item El tamaÃ±o es de 28 $\times$ 28 p\'ixeles
\item Est\'a en escala de grises de valores entre 0 (\textit{negro}) y 255 (\textit{blanco})
\end{itemize}

Uno podr\'ia argumentar que es un formato acotado, pero si uno digitaliza un texto con n\'umeros manuscritos, en general puede partirlo en im\'agenes con las condiciones pedidas. Hay escenarios en los cu\'ales este problema tiene relevancia, por nombrar algunos:

\begin{itemize}
\item Reconocimiento de textos antiguos
\item Validaci\'on digital de textos
\item Reconocimiento de caligraf\'ia
\end{itemize}

Ahora ahondaremos en los fundamentos te\'oricos de los \textit{m\'etodos num\'ericos} que servir\'an de base para el desarrollo de este trabajo.

\subsection{k Vecinos M\'as Cercanos \textit{(kNN)}}\label{intro_knn}

Un primer \textit{approach} planteado es el de \textit{\textbf{k-Nearest-Neighbours}}. Como modelos tenemos: $C$ un conjunto de clases, $Y = \{y_{1}, ..., y_{n}\}$ y $Z = \{z_{1}, ..., z_{n}\}$, $y_{i}$, $z_{i} \in \mathbb{R}^m$, dos conjuntos de elementos que pueden ser clasificados o \textit{taggeados} como elementos de $C$ con la particularidad de que conocemos la clasificaci\'on de elementos del conjunto $Z$. Por lo tanto, el problema va a ser \textit{taggear} el conjunto $Y$ en base a los de $Z$. El algoritmo es simple:

Consideramos $y \in Y$, y una funci\'on $d : Y \times Z \mapsto \mathbb{R}_{> 0}$ que denominaremos \textbf{funci\'on de distancia}. Esta funci\'on toma un elemento de $Y$ y otro de $Z$, y devuelve un valor que \textit{resume} las diferencias entre un par\'ametro y el restante a un valor. Utilizaremos la misma para calcular las ``distancias'' de $y$ contra todos los elementos $z \in Z$, quedandonos s\'olo con los $k$ de menor valor. Estos ser\'an los que m\'as se \textit{parezcan} a $y$, llamamos a este conjunto $K$.

Una vez realizado esto, sea $f : C \times Z' \subset Z \mapsto \mathbb{N}$ una funci\'on que nos devuelve la cantidad de veces que se encuentra un \textit{tag} dentro de un sub-conjunto de $Z$, tomamos $c = argmax_{c \in C} f(c, K)$. De esta forma, ``asignamos'' la clase $c$ a $y$.

Este algoritmo efectivamente realiza una \textbf{clasificaci\'on supervisada}, aprendiendo de una base de ``entrenamiento'' para categorizar los elementos restantes.

De todas formas, lo simple del algoritmo tiene su contraparte en la variable tiempo de ejecuci\'on. Uno debe aplicar $d$ sobre todos los elementos de $Z$, con lo cu\'al si el conjunto es grande, dependemos de la complejidad temporal la funci\'on de distancia. La clave de este procedimiento es elegir una funci\'on que resuma las diferencias entre elementos m\'as precisamente, minimizando el computo, lo cu\'al no es una tarea f\'acil ya que, en general, funciones de distancia livianas representan menor calidad de resultados.

En respuesta a esa problem\'atica, planteamos 2 m\'etodos denominados \textbf{An\'alisis por Componentes Principales} (\textbf{PCA}, por sus siglas en ingl\'es) y \textbf{An\'alisis Discriminante por Cuadrados M\'inimos Parciales} (\textbf{PLS-DA}). Conceptualmente, consisten en ponderar la informaci\'on para decidir qu\'e \textit{variables} almacenan m\'as informaci\'on, y usarlas para reducir el an\'alisis a ese conjunto de variables de manera tal que se mejore dram\'aticamente el tiempo de c\'omputo.

\subsection{Consideraciones previas a \textit{PCA} y \textit{PLS-DA}}\label{intro_consideraciones}

Tanto \textit{PCA} y \textit{PLS-DA} tienen como pilar realizar un \textit{cambio de base} conveniente. En cada m\'etodo var\'ia \textit{cu\'al} se busca. 

Nos abstraemos del modelo y consideramos $x_{i}$, $i = 1, ..., m$, como $m$ variables aleatorias, y $x^{j} \in \mathbb{R}^m$, $j = 1, ..., n$, como $n$ muestras de las mismas. Adem\'as, $\mu = (x^{(1)} + ... + x^{(n)}) / n \in \mathbb{R}^{m}$ la media entre las mismas. Notar que $\mu_{i} = (x^{(1)}_{i} + ... + x^{(n)}_{i}) / n$ es el promedio de la variable aleatoria $i$ y con la cu\'al podemos reescribir $\mu$ como $\mu = (\mu_{1}, ..., \mu_{n})$.

Planteamos la transformaci\'on sobre la matriz $X$, la cu\'al cuenta con $n$ filas de la forma $(x^{(i)} - \mu)^t$, $i = 1, ..., n$. Esta la matriz de datos efectivamente centra la media de las variables en $0$, lo cu\'al es simple (y se lo dejamos al lector) de verificar realizando el c\'alculo de la media sobre cada columna de $X$. Adem\'as, facilita los pasos aritm\'eticos y algebr\'aicos que se deben llevar a cabo en los m\'etodos.

La transformaci\'on la caracterizaremos como $P$ y la buscamos tal que $\hat{X}^{t} = PX^{t}$, donde $\hat{X}^{t}$ \textit{vive} en un universo de mayor conveniencia para el an\'alisis. Adicionalmente, queremos que $P$ sea ortogonal por el comportamiento que posee de no alterar el largo de los vectores a los que multiplica, para que no modifique el comportamiento de los datos.

\subsection{An\'alisis por Componentes Principales (\textit{PCA})} \label{intro_PCA}

En este primer caso, \textbf{planteamos una transformaci\'on que simult\'aneamente maximice las varianzas y minimice las covarianzas} entre las variables aleatorias. Lo que logramos con esto es reducir las dependencias, para poder analizar una variable sin considerar qu\'e ocurre con las dem\'as, y extraer la mayor cantidad de informaci\'on de cada una.

Una forma es pensarlo matricialmente. Llamamos la matriz $\mathbf{M_{X}}$ como \textbf{la matriz de covarianzas}, que posee en la posici\'on $(j, k)$ la covarianza entre las \textit{VA} $x_{j}$ y $x_{k}$, que denominamos $\sigma_{x_{j}x_{k}}$. Dicha matriz se puede obtener haciendo $M_{X} = \frac{1}{n - 1}X^{t}X$. Para verificarlo desarrollamos  $M_{X}^{(j,k)}$, con $j,k \in {1, ..., n}$: \\

$M_{X}^{(j,k)} = \frac{1}{n - 1}fila_{j}(X^t)col_{k}(X)$ \\

$\Bigg\{
\begin{array}{@{}ll@{}}
	fila_{j}(X^t) = (x^{(1)}_{j} - \mu_{j}, ..., x^{(n)}_{j} - \mu_{j})^t = ((x^{(1)}_{j}, ..., x^{(n)}_{j}) - \mu_{j}v)^{t} = (x_{j} - \mu_{j}v)^{t} \\
	col_{k}(X) = (x^{(1)}_{k} - \mu_{k}, ..., x^{(n)}_{k} - \mu_{k}) = ((x^{(1)}_{k}, ..., x^{(n)}_{k}) - \mu_{k}v) = (x_{k} - \mu_{k}v)
\end{array}$ \\

D\'onde $v = (1, ..., 1) \in \mathbb{R}^{n}$. \\

Por lo tanto,

$M_{X}^{(j,k)} = \frac{1}{n - 1}fila_{j}(X^t)col_{k}(X) = (x_{j} - \mu_{j}v)^{t}(x_{k} - \mu_{k}v) / (n - 1) = \sum\limits_{i = 1}^{n}(x^{(i)}_{j} - \mu_{j})(x^{(i)}_{k} - \mu_{k}) / (n - 1) = \sigma_{x_{j}x_{k}}$\footnote{No es la varianza \textit{te\'orica}, sino un \textit{estimador insesgado} de la misma. Dado que no an\'alisamos las variables aleatorias en t\'erminos te\'oricos sino m\'as bien en muestras, consideramos que es lo suficientemente preciso}.

Observamos adem\'as que $\sigma_{jj} = \sum\limits_{i = 1}^{n}(x_{j}^{(i)} - \mu_{j})^{2} / (n - 1) = \sigma_{j}^{2}$. Dejamos en manos del lector verificar que $M_{X}^{(j,k)} = M_{X}^{(k,j)}$. Con lo cu\'al obtuvimos una matriz que contiene a las varianzas de las variables en la diagonal y las covarianzas en el resto de las posiciones, y que es sim\'etrica. Esta \'ultima observaci\'on va a ser fundamental para los pr\'oximos pasos del procedimiento. \\

Retomemos el cambio de base y la b\'usqueda de la transformaci\'on $P$. Veamos como podemos obtenerla a partir de $M_{\hat{X}}$, la matriz de covarianzas de $\hat{X}$.

$M_{\hat{X}} = \frac{1}{n - 1}\hat{X}^{t}\hat{X} = \frac{1}{n - 1}(PX^{t})(XP^{t}) = P\frac{X^{t}X}{n - 1}P^{t} = PM_{X}P^{t}$.

Sabiendo que $M_{X}$ es sim\'etrica sabemos que \textbf{existen $\mathbf{V}$ ortogonal, cuyas columnas son sus autovectores, y $\mathbf{D}$ diagonal tal que $\mathbf{M_{X} = VDV^{t}}$} que sabemos que existen \textcolor{red}{// TODO: Insertar link a demo}. Proponemos $\mathbf{P = V^{t}}$, y arribamos a que $M_{\hat{X}} = V^{t}M_{X}V = (V^{t}V)D(VV^{t}) = D$, ya que $V$ es ortogonal. El conjunto de autovectores que forman $V$ son denominados \textbf{\textit{Componentes Principales}} de los datos.

Por lo tanto, la transformaci\'on propuesta es $P = V^{t}$. Como comentario final adicional, agregamos que este resultado se distancia completamente del concepto de clases o categor\'ias mencionado en la secci\'on anterior, por ende por s\'i mismo es un m\'etodo \textbf{\textit{no supervisado}}.

\subsection{An\'alisis Discriminante por Cuadrados M\'inimos Parciales (\textit{PLS-DA})}

\textbf{\textcolor{red}{// TODO: Fill}}